{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class LadderNet(nn.Module):\n",
    "    def __init__(self, input_shape, num_classes, rc_rate = 0.25):\n",
    "        super(LadderNet, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "        self.rc_rate = rc_rate\n",
    "\n",
    "        # Encoder layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "\n",
    "        # Decoder layers\n",
    "        self.deconv4 = nn.ConvTranspose2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(128)\n",
    "        self.deconv3 = nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.bn6 = nn.BatchNorm2d(64)\n",
    "        self.deconv2 = nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1)\n",
    "        self.bn7 = nn.BatchNorm2d(32)\n",
    "        self.deconv1 = nn.ConvTranspose2d(32, 1, kernel_size=4, padding=1)\n",
    "\n",
    "        # Classification layer\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x_labeled, x_unlabeled):\n",
    "        if (x_labeled is not None):\n",
    "\n",
    "            # Labeled input\n",
    "            h = F.relu(self.bn1(self.conv1(x_labeled)))\n",
    "            h = F.max_pool2d(h, 2, 2)\n",
    "            out1 = h\n",
    "            h = F.relu(self.bn2(self.conv2(h)))\n",
    "            h = F.max_pool2d(h, 2, 2)\n",
    "            out2 = h\n",
    "            h = F.relu(self.bn3(self.conv3(h)))\n",
    "            h = F.max_pool2d(h, 2, 2)\n",
    "            out3 = h\n",
    "            encoder_output = F.relu(self.bn4(self.conv4(h)))\n",
    "\n",
    "        else:\n",
    "\n",
    "            # Unlabeled input\n",
    "            u = F.relu(self.bn1(self.conv1(x_unlabeled)))\n",
    "            u = F.max_pool2d(u, 2, 2)\n",
    "            out1 = u\n",
    "            u = F.relu(self.bn2(self.conv2(u)))\n",
    "            u = F.max_pool2d(u, 2, 2)\n",
    "            out2 = u\n",
    "            u = F.relu(self.bn3(self.conv3(u)))\n",
    "            u = F.max_pool2d(u, 2, 2)\n",
    "            out3 = u\n",
    "            encoder_output = F.relu(self.bn4(self.conv4(u)))\n",
    "\n",
    "        encoder_output = F.avg_pool2d(encoder_output, encoder_output.size()[2:])\n",
    "        decoder_input = encoder_output\n",
    "        encoder_output = encoder_output.view(-1, 256)\n",
    "\n",
    "        # Add noise to the hidden representations\n",
    "        noise = torch.randn_like(encoder_output)\n",
    "        distorted_encoder_output = encoder_output + noise\n",
    "\n",
    "        # Decoder\n",
    "        u = F.relu(self.bn5(self.deconv4(distorted_encoder_output.view(-1, 256, 1, 1))))\n",
    "        u = F.interpolate(u, scale_factor=3, mode='nearest')\n",
    "        dout3 = u\n",
    "        u = F.relu(self.bn6(self.deconv3(u)))\n",
    "        u = F.interpolate(u, size=(7, 7), mode='nearest')\n",
    "        dout2 = u\n",
    "        u = F.relu(self.bn7(self.deconv2(u)))\n",
    "        u = F.interpolate(u, size=(14, 14), mode='nearest')\n",
    "        dout1 = u\n",
    "        u = F.interpolate(self.deconv1(u), size=(28, 28), mode='nearest')\n",
    "        x_reconstructed = torch.sigmoid(u)\n",
    "\n",
    "        # Classification\n",
    "        output = self.fc(encoder_output) if x_labeled is not None else None\n",
    "\n",
    "        return output, x_reconstructed, out1, dout1, out2, dout2, out3, dout3\n",
    "\n",
    "    def labeled_loss(self, output_labeled, target_labeled):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        return criterion(output_labeled, target_labeled)\n",
    "\n",
    "    def reconstruction_loss(self, x_reconstructed, x_unlabeled, out1, dout1, out2, dout2, out3, dout3):\n",
    "        mse_loss = nn.MSELoss()\n",
    "        mse = mse_loss(x_reconstructed, x_unlabeled) + self.rc_rate * (\n",
    "                mse_loss(out1, dout1) + mse_loss(out2, dout2) + mse_loss(out3, dout3))\n",
    "\n",
    "        return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define the transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Load the MNIST dataset\n",
    "train_set = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_set = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created\n"
     ]
    },
    {
     "data": {
      "text/plain": "device(type='cpu')"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Divide the training set into labeled and unlabeled sets\n",
    "n_labeled = int(0.1 * len(train_set))\n",
    "n_unlabeled = len(train_set) - n_labeled\n",
    "train_labeled_set, train_unlabeled_set = torch.utils.data.random_split(train_set, [n_labeled, n_unlabeled])\n",
    "\n",
    "# Create data loaders for the labeled and unlabeled sets\n",
    "batch_size = 128\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class SemisupervisedDataset(Dataset):\n",
    "    def __init__(self, labeled_dataset, unlabeled_dataset):\n",
    "        self.labeled_dataset = labeled_dataset\n",
    "        self.unlabeled_dataset = unlabeled_dataset\n",
    "        self.unlabeled_size = len(unlabeled_dataset)\n",
    "        self.labeled_size = len(labeled_dataset)\n",
    "        print(\"Dataset created\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.unlabeled_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        labeled_data, labeled_target = self.labeled_dataset[index % self.labeled_size]\n",
    "        unlabeled_data, _ = self.unlabeled_dataset[index]\n",
    "\n",
    "        return labeled_data, labeled_target, unlabeled_data\n",
    "\n",
    "\n",
    "dataset = SemisupervisedDataset(train_labeled_set, train_unlabeled_set)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "simple_labeled_dataloader = torch.utils.data.DataLoader(train_labeled_set, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "simple_labeled_test_dataloader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "# Check if a GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('CUDA is available!')\n",
    "else:\n",
    "    print('CUDA is not available.')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "def plot_history(loss, accuracy, loss_dictionary, accuracy_dictionary, name):\n",
    "    # create the accuracy plot\n",
    "    plt.subplot(2, 1, 1)  # 2 rows, 1 column, first plot\n",
    "    plt.plot(accuracy)\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "\n",
    "    # create the loss plot\n",
    "    plt.subplot(2, 1, 2)  # 2 rows, 1 column, second plot\n",
    "    plt.plot(loss)\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    # display the plots\n",
    "    plt.tight_layout()  # to prevent overlapping of labels\n",
    "    plt.show()\n",
    "\n",
    "    loss_dictionary[name] = loss.copy()\n",
    "    accuracy_dictionary[name] = accuracy.copy()\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    preds = []\n",
    "    targets = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            output, _, _, _, _, _, _, _ = model(data, None)\n",
    "            preds.extend(output.argmax(dim=1, keepdim=True).flatten().numpy())\n",
    "            targets.extend(target.numpy())\n",
    "    print(classification_report(targets, preds))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 15/422 [00:07<03:24,  1.99it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 42\u001B[0m\n\u001B[0;32m     39\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     41\u001B[0m \u001B[38;5;66;03m# Train the model on the unlabeled data\u001B[39;00m\n\u001B[1;32m---> 42\u001B[0m _, x_reconstructed, out1, dout1, out2, dout2, out3, dout3 \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata_unlabeled\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     43\u001B[0m x_reconstructed \u001B[38;5;241m=\u001B[39m x_reconstructed\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m784\u001B[39m)\n\u001B[0;32m     44\u001B[0m data_unlabeled \u001B[38;5;241m=\u001B[39m data_unlabeled\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m784\u001B[39m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[1;32mIn[1], line 54\u001B[0m, in \u001B[0;36mLadderNet.forward\u001B[1;34m(self, x_labeled, x_unlabeled)\u001B[0m\n\u001B[0;32m     49\u001B[0m     encoder_output \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbn4(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv4(h)))\n\u001B[0;32m     51\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     52\u001B[0m \n\u001B[0;32m     53\u001B[0m     \u001B[38;5;66;03m# Unlabeled input\u001B[39;00m\n\u001B[1;32m---> 54\u001B[0m     u \u001B[38;5;241m=\u001B[39m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrelu\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbn1\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_unlabeled\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     55\u001B[0m     u \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mmax_pool2d(u, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m     56\u001B[0m     out1 \u001B[38;5;241m=\u001B[39m u\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from torch import optim\n",
    "\n",
    "# Define the model\n",
    "model = LadderNet(input_shape=(1, 28, 28), num_classes=10)\n",
    "\n",
    "# Move the model to the device\n",
    "model.to(device)\n",
    "\n",
    "# Define the loss functions\n",
    "labeled_loss_fn = model.labeled_loss\n",
    "reconstruction_loss_fn = model.reconstruction_loss\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "accuracy_plot = []\n",
    "loss_plot = []\n",
    "accuracy_plots = {}\n",
    "loss_plots = {}\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for data_labeled, target_labeled, data_unlabeled in tqdm(dataloader):\n",
    "        # Train the model on the labeled data\n",
    "        data_labeled = data_labeled.to(device)\n",
    "        target_labeled = target_labeled.to(device)\n",
    "        data_unlabeled = data_unlabeled.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output, x_reconstructed, out1, dout1, out2, dout2, out3, dout3 = model(data_labeled, None)\n",
    "        loss_labeled = labeled_loss_fn(output, target_labeled) + reconstruction_loss_fn(data_labeled, x_reconstructed,\n",
    "                                                                                        out1, dout1, out2, dout2, out3,\n",
    "                                                                                        dout3)\n",
    "        loss_labeled.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Train the model on the unlabeled data\n",
    "        _, x_reconstructed, out1, dout1, out2, dout2, out3, dout3 = model(None, data_unlabeled)\n",
    "        x_reconstructed = x_reconstructed.view(-1, 784)\n",
    "        data_unlabeled = data_unlabeled.view(-1, 784)\n",
    "        loss_unlabeled = reconstruction_loss_fn(data_unlabeled, x_reconstructed, out1, dout1, out2, dout2, out3, dout3)\n",
    "        loss_unlabeled.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Test the model\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            output, _, _, _, _, _, _, _ = model(data, None)\n",
    "            test_loss += labeled_loss_fn(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    loss_plot.append(test_loss)\n",
    "    accuracy_plot.append(accuracy)\n",
    "    print('Epoch: {} Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(epoch, test_loss, correct,\n",
    "                                                                                       len(test_loader.dataset),\n",
    "                                                                                       accuracy))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_history(loss_plot, accuracy_plot, loss_plots, accuracy_plots, 'ladder with 0.25 reconstruction cost')\n",
    "evaluate_model(model, test_loader)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Звичайна модель"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Encoder layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "\n",
    "        # Classification layer\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x_labeled):\n",
    "        # Labeled input\n",
    "        h = F.relu(self.bn1(self.conv1(x_labeled)))\n",
    "        h = F.max_pool2d(h, 2, 2)\n",
    "        h = F.relu(self.bn2(self.conv2(h)))\n",
    "        h = F.max_pool2d(h, 2, 2)\n",
    "        h = F.relu(self.bn3(self.conv3(h)))\n",
    "        h = F.max_pool2d(h, 2, 2)\n",
    "        encoder_output = F.relu(self.bn4(self.conv4(h)))\n",
    "        encoder_output = F.avg_pool2d(encoder_output, encoder_output.size()[2:])\n",
    "        encoder_output = encoder_output.view(-1, 256)\n",
    "\n",
    "        # Classification\n",
    "\n",
    "        output = self.fc(encoder_output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "    def labeled_loss(self, output_labeled, target_labeled):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        return criterion(output_labeled, target_labeled)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch import optim\n",
    "\n",
    "# Define the model\n",
    "model = SimpleModel(input_shape=(1, 28, 28), num_classes=10)\n",
    "\n",
    "# Move the model to the device\n",
    "model.to(device)\n",
    "\n",
    "# Define the loss functions\n",
    "labeled_loss_fn = model.labeled_loss\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "accuracy_plot = []\n",
    "loss_plot = []\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for data_labeled, target_labeled in tqdm(simple_labeled_dataloader):\n",
    "        # Train the model on the labeled data\n",
    "        data_labeled = data_labeled.to(device)\n",
    "        target_labeled = target_labeled.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data_labeled)\n",
    "        loss_labeled = labeled_loss_fn(output, target_labeled)\n",
    "        loss_labeled.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Test the model\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in simple_labeled_test_dataloader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += labeled_loss_fn(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(simple_labeled_test_dataloader.dataset)\n",
    "    accuracy = 100. * correct / len(simple_labeled_test_dataloader.dataset)\n",
    "    loss_plot.append(test_loss)\n",
    "    accuracy_plot.append(accuracy)\n",
    "    print('Epoch: {} Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(epoch, test_loss, correct,\n",
    "                                                                                       len(test_loader.dataset),\n",
    "                                                                                       accuracy))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate_simple_model(model, test_loader):\n",
    "    preds = []\n",
    "    targets = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            output = model(data)\n",
    "            preds.extend(output.argmax(dim=1, keepdim=True).flatten().numpy())\n",
    "            targets.extend(target.numpy())\n",
    "    print(classification_report(targets, preds))\n",
    "\n",
    "plot_history(loss_plot, accuracy_plot, loss_plots, accuracy_plots, 'simple model learned only on supervised part of dataset')\n",
    "evaluate_simple_model(model, test_loader)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_accuracy_history(dictionary):\n",
    "    plt.figure()\n",
    "    for label, history in dictionary.items():\n",
    "        plt.plot(history, label=label)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.title('Accuracy History for Models')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "\n",
    "    # display the plot\n",
    "    plt.figure(figsize=(17,5))\n",
    "    plt.show()\n",
    "\n",
    "def plot_loss_history(dictionary):\n",
    "    plt.figure()\n",
    "    for label, history in dictionary.items():\n",
    "        plt.plot(history, label=label)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.title('Loss History for Models')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    # display the plot\n",
    "    plt.figure(figsize=(17,5))\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_accuracy_history(accuracy_plots)\n",
    "plot_loss_history(loss_plots)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Модель з більшою вагою reconstruction cost"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### RC rate = 0.5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = LadderNet(input_shape=(1, 28, 28), num_classes=10, rc_rate=0.5)\n",
    "\n",
    "# Move the model to the device\n",
    "model.to(device)\n",
    "\n",
    "# Define the loss functions\n",
    "labeled_loss_fn = model.labeled_loss\n",
    "reconstruction_loss_fn = model.reconstruction_loss\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "accuracy_plot = []\n",
    "loss_plot = []\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for data_labeled, target_labeled, data_unlabeled in tqdm(dataloader):\n",
    "        # Train the model on the labeled data\n",
    "        data_labeled = data_labeled.to(device)\n",
    "        target_labeled = target_labeled.to(device)\n",
    "        data_unlabeled = data_unlabeled.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output, x_reconstructed, out1, dout1, out2, dout2, out3, dout3 = model(data_labeled, None)\n",
    "        loss_labeled = labeled_loss_fn(output, target_labeled) + reconstruction_loss_fn(data_labeled, x_reconstructed,\n",
    "                                                                                        out1, dout1, out2, dout2, out3,\n",
    "                                                                                        dout3)\n",
    "        loss_labeled.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Train the model on the unlabeled data\n",
    "        _, x_reconstructed, out1, dout1, out2, dout2, out3, dout3 = model(None, data_unlabeled)\n",
    "        x_reconstructed = x_reconstructed.view(-1, 784)\n",
    "        data_unlabeled = data_unlabeled.view(-1, 784)\n",
    "        loss_unlabeled = reconstruction_loss_fn(data_unlabeled, x_reconstructed, out1, dout1, out2, dout2, out3, dout3)\n",
    "        loss_unlabeled.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Test the model\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            output, _, _, _, _, _, _, _ = model(data, None)\n",
    "            test_loss += labeled_loss_fn(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    loss_plot.append(test_loss)\n",
    "    accuracy_plot.append(accuracy)\n",
    "    print('Epoch: {} Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(epoch, test_loss, correct,\n",
    "                                                                                       len(test_loader.dataset),\n",
    "                                                                                       accuracy))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_history(loss_plot, accuracy_plot, loss_plots, accuracy_plots, 'ladder with 0.5 reconstruction cost')\n",
    "evaluate_model(model, test_loader)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### RC rate = 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = LadderNet(input_shape=(1, 28, 28), num_classes=10, rc_rate=1)\n",
    "\n",
    "# Move the model to the device\n",
    "model.to(device)\n",
    "\n",
    "# Define the loss functions\n",
    "labeled_loss_fn = model.labeled_loss\n",
    "reconstruction_loss_fn = model.reconstruction_loss\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "accuracy_plot = []\n",
    "loss_plot = []\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for data_labeled, target_labeled, data_unlabeled in tqdm(dataloader):\n",
    "        # Train the model on the labeled data\n",
    "        data_labeled = data_labeled.to(device)\n",
    "        target_labeled = target_labeled.to(device)\n",
    "        data_unlabeled = data_unlabeled.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output, x_reconstructed, out1, dout1, out2, dout2, out3, dout3 = model(data_labeled, None)\n",
    "        loss_labeled = labeled_loss_fn(output, target_labeled) + reconstruction_loss_fn(data_labeled, x_reconstructed,\n",
    "                                                                                        out1, dout1, out2, dout2, out3,\n",
    "                                                                                        dout3)\n",
    "        loss_labeled.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Train the model on the unlabeled data\n",
    "        _, x_reconstructed, out1, dout1, out2, dout2, out3, dout3 = model(None, data_unlabeled)\n",
    "        x_reconstructed = x_reconstructed.view(-1, 784)\n",
    "        data_unlabeled = data_unlabeled.view(-1, 784)\n",
    "        loss_unlabeled = reconstruction_loss_fn(data_unlabeled, x_reconstructed, out1, dout1, out2, dout2, out3, dout3)\n",
    "        loss_unlabeled.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Test the model\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            output, _, _, _, _, _, _, _ = model(data, None)\n",
    "            test_loss += labeled_loss_fn(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    loss_plot.append(test_loss)\n",
    "    accuracy_plot.append(accuracy)\n",
    "    print('Epoch: {} Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(epoch, test_loss, correct,\n",
    "                                                                                       len(test_loader.dataset),\n",
    "                                                                                       accuracy))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_history(loss_plot, accuracy_plot, loss_plots, accuracy_plots, 'ladder with 1 reconstruction cost')\n",
    "evaluate_model(model, test_loader)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Порівняння моделей"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_accuracy_history(accuracy_plots)\n",
    "plot_loss_history(loss_plots)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
